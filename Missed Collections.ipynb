{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc \n",
    "import numpy as np\n",
    "import time\n",
    "import requests, json\n",
    "from pathlib import Path  \n",
    "from datetime import date\n",
    "\n",
    "\n",
    "#datasource details removed\n",
    "# api-endpoint\n",
    " \n",
    "\n",
    "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n",
    "    columns = ['BBL','DateOpened','HouseNumber','StreetName','City','Zip','SanitationDistrict','SanitationSection']\n",
    "    values = ['DateOpened','HouseNumber','StreetName','City','Zip','SanitationDistrict','SanitationSection']\n",
    "    select = ','.join(columns)\n",
    "\n",
    "    #get data from 311 complaints and merge with df\n",
    "    sql1 = \"SELECT \"+select+\" FROM [CompostingDB].[compost].[vw_RepeatMissedCollections]\"\n",
    "    df1 = pd.read_sql(sql1,conn)\n",
    "\n",
    "    #get old data(case records) from salesforce\n",
    "    sql2 = \"SELECT \"+select+\" FROM [CompostingDB].[compost].[vw_RepeatMissedCollections_SF]\"\n",
    "    df2 = pd.read_sql(sql2,conn)\n",
    "    \n",
    "    #get data from Canvas Outreach missed collection\n",
    "    sql3 = \"SELECT \"+select+\" FROM [CompostingDB].[compost].[vw_RepeatMissedCollections_CanvasOutreach]\"\n",
    "    df3 = pd.read_sql(sql3,conn)\n",
    "    \n",
    "    frames = [df1, df2,df3]\n",
    "    df = pd.concat(frames)\n",
    "    #df.fillna('', inplace=True) \n",
    "\n",
    "    #BEGIN: fill in blank zips\n",
    "    #get non NAN zip\n",
    "    nonNanZipDf = df[~pd.isnull(df['Zip'])] \n",
    "    nonNanZipDf.drop_duplicates(\n",
    "        subset = ['BBL', 'Zip'],\n",
    "        keep = 'last',inplace=True)\n",
    "    nonNanZipDfOne = nonNanZipDf.groupby([\"BBL\",\"Zip\"]).tail(1)\n",
    "    nonNanZipDfOne2 = nonNanZipDfOne[[\"BBL\",\"Zip\"]] \n",
    "    df = df.merge(nonNanZipDfOne2, on='BBL', how='left') \n",
    "    df['Zip'] = df['Zip_y'].fillna(df['Zip_x'])\n",
    "    df.drop(['Zip_x', 'Zip_y'], axis=1,inplace=True)\n",
    "    #END: fill in blank zips\n",
    "\n",
    "    df['CombinedAddress'] = df['BBL'].astype(str) +':' + df['HouseNumber'].astype(str) +':' + df['StreetName'] +':' + df['City'] +':' + df['Zip'].astype(str)+':' + df['SanitationDistrict']+':' + df['SanitationSection']\n",
    "    \n",
    "    df.sort_values(by=['CombinedAddress', 'DateOpened'], inplace=True, ascending=True)\n",
    "    df['DateOpened'] = pd.to_datetime(df['DateOpened'])\n",
    "    df = df.loc[(df['DateOpened'] >= '2021-10-04')]\n",
    "    df.drop_duplicates(inplace=True) \n",
    "\n",
    "    #get interval between Complaint dates\n",
    "    df['DaysInterval'] = df.groupby(['CombinedAddress'])['DateOpened'].transform(lambda x: x.diff().dt.days).fillna(0) \n",
    "    \n",
    "    newColumns = ['BBL','DateOpened','DaysInterval','HouseNumber','StreetName','City','Zip','SanitationDistrict','SanitationSection','CombinedAddress']\n",
    "    df = df.reindex(columns=newColumns)\n",
    "    \n",
    "    #filter out intervals that are not 0 and 6, i.e. complaints that maybe duplicates\n",
    "    result = df[(df.DaysInterval.eq(0)) | (df.DaysInterval.ge(6))]\n",
    "    result2 = result.groupby('CombinedAddress')['CombinedAddress'].count()\n",
    "\n",
    "    #get BBLs/Complaint count of greater than 3. i.e. complaints over 3 consecutive weeks\n",
    "    result3 = result2[result2.ge(3)]\n",
    "\n",
    "    details = df[df['CombinedAddress'].isin(result3.index)]\n",
    "    details.drop_duplicates(inplace=True)\n",
    "    details['DaysInterval'] = details.groupby(['CombinedAddress'])['DateOpened'].transform(lambda x: x.diff().dt.days).fillna(0)\n",
    "\n",
    "    #filter out intervals that are not 0 and 6, i.e. complaints that maybe duplicates\n",
    "    details = details[(details.DaysInterval.eq(0)) | ( details.DaysInterval.ge(6)) ]\n",
    "\n",
    "    \n",
    "    summary = details.groupby([\"CombinedAddress\"])[\"CombinedAddress\"].count().reset_index(name=\"ComplaintCount\")  \n",
    "    \n",
    "    lastRowOfGroup = details.groupby([\"BBL\",\"HouseNumber\",\"StreetName\",\"City\",\"Zip\",\"SanitationDistrict\",\"SanitationSection\"]).tail(1)  \n",
    "    lastComplaint = lastRowOfGroup[[\"CombinedAddress\",\"DateOpened\"]]\n",
    "\n",
    "    \n",
    "    summary2 = pd.merge(summary, lastComplaint, on='CombinedAddress', how='inner')\n",
    "   \n",
    "    summary2.rename(columns={\"DateOpened\": \"LastComplaintDate\"},inplace=True)\n",
    "    summary2[['BBL','HouseNumber','StreetName','City','Zip','SanitationDistrict','SanitationSection']] = summary2.CombinedAddress.str.split(\":\",expand=True)\n",
    "    summary2.drop(['CombinedAddress'], axis=1,inplace=True)\n",
    "    \n",
    "  \n",
    "    #lastcomplaintdate to date\n",
    "    summary2['LastComplaintDate'] = summary2['LastComplaintDate'].dt.date\n",
    "    details[['BBL','HouseNumber','StreetName','City','Zip','SanitationDistrict','SanitationSection']] = details.CombinedAddress.str.split(\":\",expand=True)\n",
    "    details['DateOpened'] = details['DateOpened'].dt.date\n",
    "    details.drop(['CombinedAddress'], axis=1,inplace=True)\n",
    "\n",
    "    #remove duplicate rowsdf2 = df.drop_duplicates(subset=[\"Courses\", \"Fee\"], keep=False)\n",
    "    summary2.sort_values(by=['LastComplaintDate'], inplace=True, ascending=True)\n",
    "    summary2.drop_duplicates(subset=['BBL','HouseNumber','StreetName','City','Zip','SanitationDistrict','SanitationSection','ComplaintCount'], keep='last', inplace=True)\n",
    "    #sort from most complaints to least\n",
    "    summary2.sort_values(by=['ComplaintCount'], inplace=True, ascending=False)\n",
    "    #convert selected column to string\n",
    "    \n",
    "    summary2['HouseNumber'] = summary2['HouseNumber'].astype(str)\n",
    "    summary2['StreetName'] = summary2['StreetName'].astype(str)\n",
    "    summary2['Zip'] = summary2['Zip'].astype(str)\n",
    "\n",
    "    #Created a New Column RowId using BBL+HouseNumber+StreetName Columns\n",
    "    summary2['RowId'] = summary2[summary2.columns[2:5]].apply(\n",
    "        lambda x: ':'.join(x.dropna().astype(str)),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    \n",
    "    #summary2['CombinedAddressTempA] = summary2['BBL']+':'+summary2['HouseNumber'] +':'+ summary2['StreetName'] => join column A\n",
    "    #df1['CombinedAddressTempB'] = df1['BBL']+':'+df1['HouseNumber'] +':'+ df1['StreetName'] => join column B\n",
    "    #df1 has the Segment Id,FirstCrossStreetName, SecondCrossStreetName\n",
    "    \n",
    "    #carry over Segment Id,FirstCrossStreetName, SecondCrossStreetName to summary2 tab by joined CombinedAddressTempA & CombinedAddressTempB\n",
    "\n",
    "\n",
    "    #summary2['Latitude']= np.nan\n",
    "    #summary2['Longitude']= np.nan\n",
    "    \n",
    "    '''for index, row in summary2.iterrows():\n",
    "        params = {'AddressNo':row['HouseNumber'],'StreetName':row['StreetName'],'ZipCode':row['Zip'],'Key':apiKey} \n",
    "        try:\n",
    "            r = requests.request(method='get', url=url, params = params, proxies=proxies)\n",
    "            data = r.json()\n",
    "            recGeo = data['display']\n",
    "            summary2.at[index,'Latitude']=recGeo['out_latitude']\n",
    "            summary2.at[index,'Longitude']=recGeo['out_longitude']\n",
    "        except:\n",
    "            print(\"An exception occurred. Allowing geocode for a 3-second break...\")\n",
    "            time.sleep(3) # Sleep for 3 seconds\n",
    "            print(\"Getting back to geocoding now...\")\n",
    "    '''       \n",
    "                \n",
    "\n",
    "\n",
    "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as conn1:\n",
    "    columns1 = ['first_cross_st','second_cross_st','X_BBL','X_SEGMENT_ID']\n",
    "    values1 = ['first_cross_st','second_cross_st','X_BBL','X_SEGMENT_ID']\n",
    "\n",
    "    #get data from 311 COMPLAINTS DYNAMICS and merge with df4\n",
    "    sql4 = \"SELECT [first_cross_st] ,[second_cross_st] ,[X_BBL] ,[X_SEGMENT_ID] ,[X_BBL]+':'+[site_st_num]+':'+[site_st_name] AS RowId FROM [obi_data].[dbo].[c_311_COMPLAINTS_DYNAMICS] WHERE X_SEGMENT_ID IS NOT NULL AND ( [X_BBL]+':'+[site_st_num]+':'+[site_st_name] ) IS NOT NULL GROUP BY [X_BBL]+':'+[site_st_num]+':'+[site_st_name],[first_cross_st],[second_cross_st],[X_BBL],[X_SEGMENT_ID]\"\n",
    "    df4 = pd.read_sql(sql4,conn1)\n",
    "\n",
    "    \n",
    "    Final_df = pd.merge(summary2,df4, on='RowId')\n",
    "    del Final_df['X_BBL']\n",
    "    del Final_df['RowId']\n",
    "\n",
    "    #Final_df.columns = Final_df.columns.str.replace('ComplaintCount', 'Complaint Count')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('LastComplaintDate', 'Last Complaint Date')\n",
    "    Final_df.columns = Final_df.columns.str.replace('BBL_x', 'BBL')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('HouseNumber', 'House Number')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('StreetName', 'Street Name')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('SanitationDistrict', 'Sanitation District')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('SanitationSection', 'Sanitation Section')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('first_cross_st', 'First Cross Street')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('second_cross_st', 'Second Cross Street')\n",
    "    #Final_df.columns = Final_df.columns.str.replace('X_SEGMENT_ID', 'Segment Id')\n",
    "\n",
    "\n",
    "    #YYmmdd\n",
    "    \n",
    "    today = date.today()\n",
    "    d1 = today.strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "    writer = pd.ExcelWriter('H:'+d1+'.xlsx', engine='xlsxwriter')\n",
    "\n",
    "    Final_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "\n",
    "    summary.to_excel(writer, sheet_name='Details', index=False)\n",
    "    writer.save()\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
